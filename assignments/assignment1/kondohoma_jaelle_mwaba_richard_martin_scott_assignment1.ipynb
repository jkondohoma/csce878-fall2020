{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "## K Nearest Neighbors Model for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pandas.plotting import scatter_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 & 2: Euclidean and Manhattan Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_dist(v: np.array, w: np.array, p: int) -> float:\n",
    "    \"\"\"Calculate the or L_p distance between 2 vectors v and w for an integer p\"\"\"\n",
    "    difference = abs(v - w)\n",
    "    sumVals = sum(pow(difference, p))\n",
    "    return pow(sumVals, 1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(v: np.array, w: np.array) -> float:\n",
    "    \"\"\"Calculate the euclidean distance between 2 vectors v and w\"\"\"\n",
    "    return minkowski_dist(v,w,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_dist(v: np.array, w: np.array) -> float:\n",
    "    \"\"\"Calcuate the Manhattan (or L_1) distance between 2 vectors v and w\"\"\"\n",
    "    return minkowski_dist(v,w,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Accuracy and Generalization Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(vals: np.array, predictions: np.array) -> float:\n",
    "    \"\"\"Calculate the accuracy between the vals vector and the predictions vector\"\"\"\n",
    "    return np.mean(vals == predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalization_error(vals: np.array, predictions: np.array)-> float:\n",
    "    \"\"\"Calculate the generalization error between the vals and predictions vector\"\"\"\n",
    "    return 1-accuracy(vals, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Precision, Recall, F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionCountValues:\n",
    "    \"\"\"Stores the count values of predicted results and calculates precision and recall\"\"\"\n",
    "    def __init__(self, true_positives, false_positives, true_negatives, false_negatives):\n",
    "        self.true_positives = true_positives\n",
    "        self.false_positives = false_positives\n",
    "        self.true_negatives = true_negatives\n",
    "        self.false_negatives = false_negatives\n",
    "    \n",
    "    def precision(self) -> float:\n",
    "        \"\"\"Calculate precision for the given prediction count values\"\"\"\n",
    "        return self.true_positives/(self.true_positives+self.false_positives)\n",
    "\n",
    "    def recall(self) -> float:\n",
    "        \"\"\"Calculate recall for the given prediction count values\"\"\"\n",
    "        return self.true_positives/(self.true_positives+self.false_negatives)\n",
    "    \n",
    "    def precision_and_recall(self) -> (float, float):\n",
    "        \"\"\"Return precision and recall in a tuple (in that order) for the given prediction count values\"\"\"\n",
    "        return (self.precision(), self.recall())\n",
    "        \n",
    "def prediction_count_values(vals: np.array, predicted_probs: np.array, threshold=1) -> PredictionCountValues:\n",
    "    \"\"\"\n",
    "    Determine number of false postive, false negatives, true positives, true negatives for a given threshold and values\n",
    "    Parameters: \n",
    "        vals-target values\n",
    "        predicted_props-predicted probabilities of target values\n",
    "        threshold-the threshold at which prediced_probs must be to classify as true. Defaults to 1 to accomodate true predicted vals\n",
    "    Returns:\n",
    "        ScoreValueResult containing (# true positives, # false positives, # true negatives, # false negatives)\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    for idx in range(len(vals)):\n",
    "        if predicted_probs[idx] >= threshold:\n",
    "            if vals[idx]:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        else:\n",
    "            if vals[idx]:\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "    return PredictionCountValues(true_positives, false_positives, true_negatives, false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(vals: np.array, predictions: np.array)-> float:\n",
    "    \"\"\"Calculate the precision=(true positives/total predicted positives) for the vals and predictions vectors\"\"\"\n",
    "    scores = prediction_count_values(vals, predictions)\n",
    "    return scores.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(vals: np.array, predictions: np.array)-> float:\n",
    "    \"\"\"Calculate the recall=(true positives/total positives) for the vals and predictions vectors\"\"\"\n",
    "    scores = prediction_count_values(vals, predictions)\n",
    "    return scores.recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(vals: np.array, predictions: np.array)-> float:\n",
    "    \"\"\"Calculate the F1 Score = harmonic_mean(precision, recall)\"\"\"\n",
    "    # Technically looping over array twice unnecessarily, could be more efficient, but okay for now\n",
    "    scores = prediction_count_values(vals, predictions)\n",
    "    prec, rec = scores.precision_and_recall()\n",
    "    return (2*prec*rec)/(prec+rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(vals: np.array, predictions: np.array)-> np.array:\n",
    "    \"\"\"Calculate the confusion matrix for the given values and predictions\"\"\"\n",
    "    scores = prediction_count_values(vals, predictions)\n",
    "    return np.array([[scores.true_negatives, scores.false_positives],[scores.false_negatives, scores.true_positives]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thresholds(min_val: int, max_val: int, threshold_step_size: float)-> np.array:\n",
    "    \"\"\"Generate threshold values of a given step size from min_val to max_val always including both min and max vals\"\"\"\n",
    "    return np.append(np.arange(min_val,max_val,threshold_step_size), [max_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Generate ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(vals: np.array, pred_score_probs: np.array, threshold: float)->(float, float):\n",
    "    \"\"\"Calculate TPR and FPR and return as tuple (tpr, fpr)\"\"\"\n",
    "    scores = prediction_count_values(vals, pred_score_probs, threshold)\n",
    "    tpr = scores.true_positives/(scores.true_positives+scores.false_negatives)\n",
    "    fpr = scores.false_positives/(scores.false_positives+scores.true_negatives)\n",
    "    return (tpr, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(vals: np.array, pred_scores: np.array, threshold_step_size: float = 0.0005) -> (np.array, np.array, np.array):\n",
    "    # NOTE: scikitlearn I think does some fancy things to calculate appropriate thresholds...we'll just use a set step size\n",
    "    max_threshold = 2.0\n",
    "    thresholds = np.append(generate_thresholds(0,1,threshold_step_size), [max_threshold])\n",
    "    thresholds = np.flip(thresholds)\n",
    "    tprs = np.empty(len(thresholds))\n",
    "    fprs = np.empty(len(thresholds))\n",
    "    for idx, threshold in enumerate(thresholds):\n",
    "        tpr, fpr = calculate_tpr_fpr(vals, pred_scores, threshold)\n",
    "        tprs[idx] = tpr\n",
    "        fprs[idx] = fpr\n",
    "    return (tprs, fprs, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(true_positive_rate: np.array, false_positive_rate: np.array, title='ROC Curve', label=None)-> None:\n",
    "    plt.style.use('ggplot')\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(false_positive_rate, true_positive_rate, color='darkorange', linewidth=7, label=label)\n",
    "    plt.plot([0,1], [0,1], color='black', lw=2, linestyle='--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Compute Area Under Curve for ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score(y_vals: np.array, y_predicted_probs: np.array) -> float:\n",
    "    \"\"\"Calculate AUC of the ROC for the given y_vals and y_predicted_probs using the trapezoidal rule\"\"\"\n",
    "    auc = 0\n",
    "    tprs, fprs, _ = roc_curve(y_vals, y_predicted_probs)\n",
    "    # Uses the trapezoidal rule for calculating AUC\n",
    "    for idx in range(len(tprs)-1):\n",
    "        average_tpr = (tprs[idx]+tprs[idx+1])/2\n",
    "        delta_fpr = fprs[idx+1]-fprs[idx]\n",
    "        trap_area = average_tpr*delta_fpr\n",
    "        auc += trap_area\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Generate Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(vals: np.array, predicted_scores: np.array, threshold_step_size=0.0005) -> (np.array, np.array, np.array):\n",
    "    \"\"\"Calculate precision recall curve and return precisions, recalls, thresholds\"\"\"\n",
    "    thresholds = generate_thresholds(0,1,threshold_step_size)\n",
    "    precisions = np.empty(len(thresholds))\n",
    "    recalls = np.empty(len(thresholds))\n",
    "    thresholds = np.flip(thresholds) # Reverse thresholds array\n",
    "    for idx, threshold in enumerate(thresholds):\n",
    "        scores = prediction_count_values(vals, predicted_scores, threshold)\n",
    "        precisionScore, recallScore = scores.precision_and_recall()\n",
    "        precisions[idx] = precisionScore\n",
    "        recalls[idx] = recallScore\n",
    "    return (precisions, recalls, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(precisions: np.array, recalls: np.array, thresholds: np.array, title='Precision-Recall Curve') -> None:\n",
    "    plt.style.use('ggplot')\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(thresholds, precisions, 'b--', linewidth=8, label='Precision')\n",
    "    plt.plot(thresholds, recalls, 'g-', linewidth=3, label='Recall')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.title(title)\n",
    "    plt.ylim([0,1.1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: KNN_Classifier Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifier:\n",
    "    \"\"\"KNN classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X: np.array, Y: np.array, n_neighbors: int, weights = 'uniform', **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the KNN classifier values. Note p is the power on the Minkowski metric\n",
    "        NOTE: If you want to use a specific metric put it in kwargs with the metric parameter\n",
    "        Currently implemented metrics are manhattan and euclidean\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.kwargs = kwargs\n",
    "        self.metric = manhattan_dist if ('metric' in kwargs and kwargs['metric'] == 'manhattan') else euclidean_dist\n",
    "    \n",
    "    def predict(self, X: np.array)-> np.array:\n",
    "        \"\"\"Predict the classification of each input in given ndarray\"\"\"\n",
    "        preds = np.empty(X.shape[0])\n",
    "        for idx, x in enumerate(X):\n",
    "            neighborhood = self.__find_neighbors(x)\n",
    "            neighbors = [(distance, self.Y[neighbor]) for distance, neighbor in neighborhood]\n",
    "            if self.weights is 'distance':\n",
    "                class1_score = 0\n",
    "                class2_score = 0\n",
    "                for dist, y in neighbors:\n",
    "                    if y:\n",
    "                        class1_score += (1/dist)\n",
    "                    else:\n",
    "                        class2_score += (1/dist)\n",
    "                preds[idx] = 1 if class1_score > class2_score else 0\n",
    "            else:\n",
    "                nearest_nei_total_true = sum(val for _, val in neighbors)\n",
    "                preds[idx] = 1 if nearest_nei_total_true > (self.n_neighbors/2) else 0\n",
    "        return preds\n",
    "            \n",
    "    def __find_neighbors(self, sample: np.array) -> list:\n",
    "        \"\"\"Find list of nearest neighbors\"\"\"\n",
    "        distances = []\n",
    "        # Note: could try to optimize and keep track of nearest neighbors as you go but ends up with lots of operations on neighbors list\n",
    "        for idx, data in enumerate(self.X):\n",
    "            distance = self.metric(sample, data)\n",
    "            distances.append((distance, idx))\n",
    "        return sorted(distances)[:self.n_neighbors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Read in the wine quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering shipping the dataset with the code and creating a function that reads it from its location\n",
    "wine_df = pd.read_csv('./data/winequality-white.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Convert target column to two-category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df['quality'] = (wine_df['quality'] > 5).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12: Summarize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\ncount    4898.000000       4898.000000  4898.000000     4898.000000   \nmean        6.854788          0.278241     0.334192        6.391415   \nstd         0.843868          0.100795     0.121020        5.072058   \nmin         3.800000          0.080000     0.000000        0.600000   \n25%         6.300000          0.210000     0.270000        1.700000   \n50%         6.800000          0.260000     0.320000        5.200000   \n75%         7.300000          0.320000     0.390000        9.900000   \nmax        14.200000          1.100000     1.660000       65.800000   \n\n         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\ncount  4898.000000          4898.000000           4898.000000  4898.000000   \nmean      0.045772            35.308085            138.360657     0.994027   \nstd       0.021848            17.007137             42.498065     0.002991   \nmin       0.009000             2.000000              9.000000     0.987110   \n25%       0.036000            23.000000            108.000000     0.991723   \n50%       0.043000            34.000000            134.000000     0.993740   \n75%       0.050000            46.000000            167.000000     0.996100   \nmax       0.346000           289.000000            440.000000     1.038980   \n\n                pH    sulphates      alcohol      quality  \ncount  4898.000000  4898.000000  4898.000000  4898.000000  \nmean      3.188267     0.489847    10.514267     0.665169  \nstd       0.151001     0.114126     1.230621     0.471979  \nmin       2.720000     0.220000     8.000000     0.000000  \n25%       3.090000     0.410000     9.500000     0.000000  \n50%       3.180000     0.470000    10.400000     1.000000  \n75%       3.280000     0.550000    11.400000     1.000000  \nmax       3.820000     1.080000    14.200000     1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n      <td>4898.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>6.854788</td>\n      <td>0.278241</td>\n      <td>0.334192</td>\n      <td>6.391415</td>\n      <td>0.045772</td>\n      <td>35.308085</td>\n      <td>138.360657</td>\n      <td>0.994027</td>\n      <td>3.188267</td>\n      <td>0.489847</td>\n      <td>10.514267</td>\n      <td>0.665169</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.843868</td>\n      <td>0.100795</td>\n      <td>0.121020</td>\n      <td>5.072058</td>\n      <td>0.021848</td>\n      <td>17.007137</td>\n      <td>42.498065</td>\n      <td>0.002991</td>\n      <td>0.151001</td>\n      <td>0.114126</td>\n      <td>1.230621</td>\n      <td>0.471979</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.800000</td>\n      <td>0.080000</td>\n      <td>0.000000</td>\n      <td>0.600000</td>\n      <td>0.009000</td>\n      <td>2.000000</td>\n      <td>9.000000</td>\n      <td>0.987110</td>\n      <td>2.720000</td>\n      <td>0.220000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.300000</td>\n      <td>0.210000</td>\n      <td>0.270000</td>\n      <td>1.700000</td>\n      <td>0.036000</td>\n      <td>23.000000</td>\n      <td>108.000000</td>\n      <td>0.991723</td>\n      <td>3.090000</td>\n      <td>0.410000</td>\n      <td>9.500000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6.800000</td>\n      <td>0.260000</td>\n      <td>0.320000</td>\n      <td>5.200000</td>\n      <td>0.043000</td>\n      <td>34.000000</td>\n      <td>134.000000</td>\n      <td>0.993740</td>\n      <td>3.180000</td>\n      <td>0.470000</td>\n      <td>10.400000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.300000</td>\n      <td>0.320000</td>\n      <td>0.390000</td>\n      <td>9.900000</td>\n      <td>0.050000</td>\n      <td>46.000000</td>\n      <td>167.000000</td>\n      <td>0.996100</td>\n      <td>3.280000</td>\n      <td>0.550000</td>\n      <td>11.400000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>14.200000</td>\n      <td>1.100000</td>\n      <td>1.660000</td>\n      <td>65.800000</td>\n      <td>0.346000</td>\n      <td>289.000000</td>\n      <td>440.000000</td>\n      <td>1.038980</td>\n      <td>3.820000</td>\n      <td>1.080000</td>\n      <td>14.200000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13: Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_shuffled_df = wine_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14: Generate pair plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just used the code from the recitation. I'm not sure if this is a problem and if using scipy is allowed. If it is a problem, I'm going to redo this part \n",
    "#If it's not a problem, I will have to write a proper citation right here\n",
    "# Calculate correlation coefficient\n",
    "def corrfunc(x, y, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax = plt.gca()\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .6), xycoords=ax.transAxes,\n",
    "               size = 24)\n",
    "    \n",
    "cmap = sns.cubehelix_palette(light=1, dark = 0.1,\n",
    "                             hue = 0.5, as_cmap=True)\n",
    "\n",
    "sns.set_context(font_scale=2)\n",
    "\n",
    "# Pair grid set up\n",
    "g = sns.PairGrid(wine_shuffled_df)\n",
    "\n",
    "# Scatter plot on the upper triangle\n",
    "g.map_upper(plt.scatter, s=10, color = 'red')\n",
    "\n",
    "# Distribution on the diagonal\n",
    "g.map_diag(sns.distplot, kde=False, color = 'red')\n",
    "\n",
    "# Density Plot and Correlation coefficients on the lower triangle\n",
    "g.map_lower(sns.kdeplot, cmap = cmap)\n",
    "g.map_lower(corrfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 15: Drop redudant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'density' and 'total sulfur dioxide' in recitation 3 gave the best overall statistics\n",
    "wine_shuffled_df = wine_shuffled_df.drop(columns=['density', 'total sulfur dioxide'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 16: Create partition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(features: np.array, target: np.array, part_size: int) -> np.array:\n",
    "    length = features.shape[0]\n",
    "    part = round(part_size * length)\n",
    "    part_arr = np.random.choice(range(length), part, False)\n",
    "\n",
    "    index_arr = np.zeros(length, dtype=bool)\n",
    "    index_arr[part_arr] = True\n",
    "\n",
    "    X_test = features[index_arr]\n",
    "    X_train = features[~index_arr]\n",
    "    y_test = target[index_arr]\n",
    "    y_train = target[~index_arr]\n",
    "\n",
    "    return X_train, X_test, y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(features: np.array) -> np.array:\n",
    "    X_mean = np.mean(features, axis=0)\n",
    "    X_mean = np.reshape(X_mean, X_mean.shape[0])\n",
    "    X_std = np.std(features, axis=0)\n",
    "    X_std = np.reshape(X_std, X_std.shape[0])\n",
    "\n",
    "    X_standardized = (features - X_mean) / X_std\n",
    "\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 17: Run KNN_Classifier model on train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtasks a, b , c and d: Run KNN_Classifier using uniform weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features and label matrices\n",
    "X = wine_df.drop('quality', axis=1)\n",
    "y = wine_df['quality']\n",
    "\n",
    "#Partition the data into X_train, X_test, y_train and y_test\n",
    "X_train, X_test, y_train, y_test = partition(X.values, y.values, part_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F1 Score: 0.85\n"
    }
   ],
   "source": [
    "# Run KNN on non-standardized data and compute the F1 Score\n",
    "knn = KNN_Classifier()\n",
    "knn.fit(X_train, y_train, n_neighbors=5)\n",
    "\n",
    "y_predicted = knn.predict(X_train)\n",
    "\n",
    "print(\"F1 Score: %0.2f\" % f1_score(y_train, y_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data and partition it into X_train, X_test, y_train and y_test \n",
    "X = standardize_features(X.values)\n",
    "X_train, X_test, y_train,y_test = partition(X, y.values, part_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Runtime: 99.68052124977112\nF1 Score: 0.88\n"
    }
   ],
   "source": [
    "# Run KNN on standardized data and compute the F1 Score\n",
    "knn = KNN_Classifier()\n",
    "knn.fit(X_train, y_train, n_neighbors=5)\n",
    "\n",
    "y_predicted = knn.predict(X_train)\n",
    "\n",
    "print(\"F1 Score: %0.2f\" % f1_score(y_train, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask e: Run KNN_Classifier using distance weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features and label matrices\n",
    "X = wine_df.drop('quality', axis=1)\n",
    "y = wine_df['quality']\n",
    "\n",
    "#Partition the data into X_train, X_test, y_train and y_test\n",
    "X_train, X_test, y_train, y_test = partition(X.values, y.values, part_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F1 Score: 1.00\n"
    }
   ],
   "source": [
    "# Run KNN on non-standardized data and compute the F1 Score\n",
    "knn = KNN_Classifier()\n",
    "knn.fit(X_train, y_train, n_neighbors=5, weights='distance')\n",
    "\n",
    "y_predicted = knn.predict(X_train)\n",
    "\n",
    "print(\"F1 Score: %0.2f\" % f1_score(y_train, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data and partition it into X_train, X_test, y_train and y_test \n",
    "X = standardize_features(X.values)\n",
    "X_train, X_test, y_train,y_test = partition(X, y.values, part_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F1 Score: 1.00\n"
    }
   ],
   "source": [
    "# Run KNN on standardized data and compute the F1 Score\n",
    "knn = KNN_Classifier()\n",
    "knn.fit(X_train, y_train, n_neighbors=5, weights='distance')\n",
    "\n",
    "y_predicted = knn.predict(X_train)\n",
    "\n",
    "print(\"F1 Score: %0.2f\" % f1_score(y_train, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F1 Score: 0.87\n"
    }
   ],
   "source": [
    "knn = KNN_Classifier()\n",
    "knn.fit(X_train, y_train, n_neighbors=5, weights='distance')\n",
    "y_predicted = knn.predict(X_test)\n",
    "print(\"F1 Score: %0.2f\" % f1_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Things Below (DELETE BEFORE SUBMISSION)\n",
    "\n",
    "### Just simple non-rigorous sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VERY QUICK NOT RIGOROUS TEST CELL\n",
    "y_vals = np.array([1,0,1,1,0,0,0,1,1,0,0,1,1,1])\n",
    "y_preds = np.array([0,1,1,1,0,0,0,1,0,1,0,0,1,0])\n",
    "print(len(y_vals))\n",
    "print(accuracy(y_vals,y_preds))\n",
    "print(generalization_error(y_vals, y_preds))\n",
    "print(precision(y_vals, y_preds))\n",
    "print(recall(y_vals, y_preds))\n",
    "print(f1_score(y_vals, y_preds))\n",
    "print(confusion_matrix(y_vals,y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from reci3 to get test data\n",
    "from pathlib import Path \n",
    "test_data = Path('/Users/scott.martin/Documents/unl_box/Box Sync/courses/csce878_machine_learning/recitations/recitation3')\n",
    "y_scores = []\n",
    "with open(test_data/'y_scores.csv') as file:\n",
    "    for line in file:\n",
    "        y_scores.append(float(line.strip()))\n",
    "    \n",
    "y_vals = []\n",
    "with open(test_data/'y_values.csv') as file:\n",
    "    for line in file:\n",
    "        y_vals.append(float(line.strip()))\n",
    "\n",
    "y_scores = np.array(y_scores)\n",
    "y_vals = np.array(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr, thresholds = roc_curve(y_vals, y_scores) # This is SOOOOOO SLOW!!! Speed it up...I'm being dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(tpr, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_vals,y_scores) # Still hideously slow...but okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_tests, recall_tests, new_thresh = precision_recall_curve(y_vals, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(precision_tests, recall_tests, new_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(test_data/'winequality-white.csv', delimiter=';')\n",
    "target = (df['quality']>5).astype(np.int)\n",
    "df = df.drop(columns=['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "x_train, x_test, y_train, y_test = train_test_split(df, target)\n",
    "x_train = scale(x_train)\n",
    "x_test = scale(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN_Classifier()\n",
    "knn.fit(x_train, y_train.values, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_x_test = x_test\n",
    "rsm_y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = knn.predict(rsm_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(accuracy(rsm_y_test, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}